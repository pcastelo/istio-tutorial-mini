= Java (Spring Boot) + Istio on Kubernetes/OpenShift
// Settings:
:idprefix:
:idseparator: -
ifndef::env-github[]
:icons: font
endif::[]
ifdef::env-github,env-browser[]
:toc: preamble
:toclevels: 1
endif::[]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:!toc-title:
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]

// URIs:
ifdef::env-github[]
:uri-repo-file-prefix: link:
:uri-repo-tree-prefix: link:
endif::[]

[TIP]
====
If you are in a hurry and want to get hands-on with Istio insanely fast, just go to http://learn.openshift.com/servicemesh[http://learn.openshift.com/servicemesh] and start instantly
====

There are three different and super simple microservices in this system and they are chained together in the following sequence:

```
customer → preference → recommendation
```

For now, they have a simple exception handling solution for dealing with a missing dependent service: it just returns the error message to the end-user.

== Prerequisite CLI tools

You will need in this tutorial:

* `minishift` 
** https://github.com/minishift/minishift/releases[Mac OS and Fedora]
* docker
** https://www.docker.com/docker-mac[Mac OS]
** Fedora: `dnf install docker`
* kubectl
** https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-binary-via-curl[Mac OS]
** Fedora: `dnf install kubernetes-client`
* `oc (eval $(minishift oc-env))`
* Apache Maven
** https://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz[Mac OS]
** Fedora: `dnf install maven`
* link:https://github.com/wercker/stern[stern]
** Mac OS: `brew install stern`
** Fedora: `sudo curl --output /usr/local/bin/stern -L https://github.com/wercker/stern/releases/download/1.6.0/stern_linux_amd64 && sudo chmod +x /usr/local/bin/stern`
* istioctl (will be installed via the steps below)
* `curl`, `gunzip`, `tar` 
** Mac OS: built-in or part of your bash shell
** Fedora: should also be installed already, but just in case... `dnf install curl gzip tar`
* git
** `dnf install git`

== Setup minishift

Assumes `minishift`, tested with minishift v1.15.1+a5c47dd

[source,bash]
----
#!/bin/bash

# add the location of minishift execuatable to PATH
# I also keep other handy tools like kubectl and kubetail.sh
# in that directory

minishift profile set kubeboot
minishift config set memory 8GB
minishift config set cpus 3
minishift config set vm-driver virtualbox ## or kvm, for Fedora
minishift config set image-caching true
minishift addon enable admin-user

minishift start
----

== Setup environment

[source,bash]
----
eval $(minishift oc-env)  && eval $(minishift docker-env)
oc login $(minishift ip):8443 -u admin -p admin
----

NOTE: In this tutorial, you will often be polling the customer endpoint with `curl`, while simultaneously viewing logs via `stern` or `kubetail.sh` and issuing commands via `oc` and `istioctl`. Consider using three terminal windows.

== Istio installation script

[source,bash]
----
#!/bin/bash

# Mac OS:
curl -L https://github.com/istio/istio/releases/download/1.0.0/istio-1.0.0-osx.tar.gz | tar xz

# Fedora:
curl -L https://github.com/istio/istio/releases/download/1.0.0/istio-1.0.0-linux.tar.gz | tar xz

# Both:
cd istio-1.0.0
export ISTIO_HOME=`pwd`
export PATH=$ISTIO_HOME/bin:$PATH
----

Verify **istioctl**

[source,bash]
----
istioctl version 
----

The above command should show output like:

```
Version: 1.0.0
GitRevision: 3a136c90ec5e308f236e0d7ebb5c4c5e405217f4
User: root@71a9470ea93c
Hub: gcr.io/istio-release
GolangVersion: go1.10.1
BuildStatus: Clean
```

[source,bash]
----
oc login $(minishift ip):8443 -u admin -p admin
oc adm policy add-scc-to-user anyuid -z istio-ingress-service-account -n istio-system
oc adm policy add-scc-to-user anyuid -z default -n istio-system
oc adm policy add-scc-to-user anyuid -z prometheus -n istio-system
oc adm policy add-scc-to-user anyuid -z istio-egressgateway-service-account -n istio-system
oc adm policy add-scc-to-user anyuid -z istio-citadel-service-account -n istio-system
oc adm policy add-scc-to-user anyuid -z istio-ingressgateway-service-account -n istio-system
oc adm policy add-scc-to-user anyuid -z istio-cleanup-old-ca-service-account -n istio-system
oc adm policy add-scc-to-user anyuid -z istio-mixer-post-install-account -n istio-system
oc adm policy add-scc-to-user anyuid -z istio-mixer-service-account -n istio-system
oc adm policy add-scc-to-user anyuid -z istio-pilot-service-account -n istio-system
oc adm policy add-scc-to-user anyuid -z istio-sidecar-injector-service-account -n istio-system
oc adm policy add-scc-to-user anyuid -z istio-galley-service-account -n istio-system
oc create -f install/kubernetes/istio-demo.yaml
oc project istio-system
oc expose svc istio-ingress
oc apply -f install/kubernetes/addons/prometheus.yaml
oc apply -f install/kubernetes/addons/grafana.yaml
oc apply -f install/kubernetes/addons/servicegraph.yaml
oc expose svc servicegraph
oc expose svc grafana
oc expose svc prometheus
oc expose svc jaeger-query
----

Wait for Istio's components to be ready

[source,bash]
----
$ oc get pods -w -n istio-system
NAME                                        READY     STATUS      RESTARTS   AGE
grafana-6995b4fbd7-2m2sc                    1/1       Running     0          1h
istio-citadel-54f4678f86-dd75x              1/1       Running     0          1h
istio-cleanup-secrets-57nmm                 0/1       Completed   0          1h
istio-egressgateway-5d7f8fcc7b-8x2bz        1/1       Running     0          1h
istio-galley-7bd8b5f88f-8vwgv               1/1       Running     0          1h
istio-grafana-post-install-v4s4j            0/1       Completed   0          1h
istio-ingressgateway-6f58fdc8d7-br4gw       1/1       Running     0          1h
istio-pilot-d99689994-r275d                 2/2       Running     0          1h
istio-policy-766bf4bd6d-wksl8               2/2       Running     0          1h
istio-sidecar-injector-85ccf84984-kxdhb     1/1       Running     0          1h
istio-statsd-prom-bridge-55965ff9c8-8s5j5   1/1       Running     0          1h
istio-telemetry-55b6b5bbc7-vcv9b            2/2       Running     0          1h
istio-tracing-77f9f94b98-th89f              1/1       Running     0          1h
prometheus-7456f56c96-6m6h7                 1/1       Running     0          1h
servicegraph-684c85ffb9-pxdrl               1/1       Running     0          1h
----

And if you need quick access to the OpenShift console

[source,bash]
----
minishift console
----

NOTE: On your first launch of the OpenShift console via `minishift`, you will receive a warning like "Your connection is not private". For our demo, simply select "Proceed to 192.168.xx.xx (unsafe)" to bypass the warning. Both the username and the password are set to `admin`, thanks to the `admin-user` add-on.

== Essential URLS

* OpenShift Web Console - `minishift console`
* Jaeger - `minishift openshift service jaeger-query --in-browser`
* Grafana - `minishift openshift service grafana --in-browser`
* Prometheus - `minishift openshift service prometheus --in-browser`

NOTE: You can see more options via `minishift openshift service --help`

== Set Environment

Make sure you are logged in

[source,bash]
----
oc whoami
----

and you have setup the project/namespace

[source,bash]
----
oc new-project tutorial
oc adm policy add-scc-to-user privileged -z default -n tutorial
----

Then clone the git repository

[source,bash]
----
git clone https://github.com/workspace7/kubeboot
cd kubeboot
----

== Deploy customer

Start deploying the microservice projects, starting with customer

[source,bash]
----
cd customer
mvn clean package
docker build -t example/customer .
docker images | grep customer
----

NOTE: Your very first Docker build will take a bit of time as it downloads all the layers. Subsequent rebuilds of the Docker image, updating only the microservice layer will be very fast.

Make sure `istioctl` is in your `PATH`:

Now let's deploy the customer pod:

[source,bash]
----
oc apply -f ./kubernetes/Deployment.yml -n tutorial
oc create -f ./kubernetes/Service.yml -n tutorial
oc create -f ./kubernetes/gateway.yml -n tutorial
----

Since the `customer` service is the one our users will interact with, let's add an OpenShift Route that exposes that endpoint.

[source,bash]
----
oc expose service customer
oc get route
oc get pods -w
----

IMPORTANT: If your pod fails with `ImagePullBackOff`, it's possible that your current terminal isn't using the proper Docker Environment. See link:#setup-environment[Setup environment].

Wait until the status is `Running` and there are `1/1` pods in the `Ready` column. To exit, press `Ctrl+C`.  You can also see the pod status via OpenShift WebConsole.

Then test the customer endpoint

[source,bash]
----
curl customer-tutorial.$(minishift ip).nip.io/whereami
----

The output of the above command should be something like `customer => customer-5dffb8bff9-qntf2c`

[source,bash]
----
curl customer-tutorial.$(minishift ip).nip.io
----

You should see the following error because the services `preference` and `recommendation` are not yet deployed.

----
customer => I/O error on GET request for "http://preference:8080": preference: Name or service not known; nested exception is java.net.UnknownHostException: preference: Name or service not known
----

Also review the logs

[source,bash]
----
stern customer
----

You should see a stacktrace containing this cause:

[source,bash]
----
org.springframework.web.client.ResourceAccessException: I/O error on GET request for "http://preference:8080": preference; nested exception is java.net.UnknownHostException: preference
----

Back to the main kubeboot directory

[source,bash]
----
cd ..
----

== Deploy preference

[source,bash]
----
cd preference
mvn clean package
docker build -t example/preference .
docker images | grep preference
oc apply -f  ./kubernetes/Deployment.yml -n tutorial
oc create -f ./kubernetes/Service.yml -n tutorial
oc get pods -w
----

Wait until the status is `Running` and there are `1/1` pods in the `Ready` column. To exit, press `Ctrl+C`

[source,bash]
----
curl customer-tutorial.$(minishift ip).nip.io
----

It will respond with an error since the service `recommendation` is not yet deployed.

NOTE: We could make this a bit more resilent in a future iteration of this tutorial

[source,bash]
----
customer => 503 preference => I/O error on GET request for "http://recommendation:8080": recommendation: Name or service not known; nested exception is java.net.UnknownHostException: recommendation: Name or service not known
----

and check out the logs

[source,bash]
----
stern preference
----

You should see a stacktrace containing this cause:

[source,bash]
----
org.springframework.web.client.ResourceAccessException: I/O error on GET request for "http://recommendation:8080": recommendation; nested exception is java.net.UnknownHostException: recommendation
----

Back to the main kubeboot directory

[source,bash]
----
cd ..
----

== Deploy recommendation

IMPORTANT: The tag `v1` at the end of the image name matters. We will be creating a `v2` version of `recommendation` later in this tutorial. Having both a `v1` and `v2` version of the `recommendation` code will allow us to exercise some interesting aspects of Istio's capabilities.

[source,bash]
----
cd recommendation
mvn clean package
docker build -t example/recommendation:v1 .
docker images | grep recommendation
oc apply -f  ./kubernetes/Deployment.yml -n tutorial
oc create -f ./kubernetes/Service.yml -n tutorial
oc get pods -w
----

Wait until the status is `Running` and there are `1/1` pods in the `Ready` column. To exit, press `Ctrl+C`

[source,bash]
----
curl customer-tutorial.$(minishift ip).nip.io
----

it should now return

[source,bash]
----
customer => preference => recommendation v1 from '99634814-sf4cl': 1
----

and you can monitor the `recommendation` logs with

[source,bash]
----
stern recommendation -c recommendation
----

Back to the main `kubeboot` directory

[source,bash]
----
cd ..
----

== Updating Redeploying Code

When you wish to change code (e.g. editing the .java files) and wish to "redeploy", simply:

[source,bash]
----
cd {servicename}/

vi src/main/java/com/redhat/developer/demos/{servicename}/{Servicename}{Controller}.java
----

Make your changes, save it and then:

[source,bash]
----
mvn clean package
docker build -t example/{servicename} .
oc get pods -o jsonpath='{.items[*].metadata.name}' -l app={servicename}
oc get pods -o jsonpath='{.items[*].metadata.name}' -l app={servicename},version=v1

oc delete pod -l app={servicename},version=v1
----

Why the delete pod?

Based on the Deployment configuration, Kubernetes/OpenShift will recreate the pod, based on the new docker image as it attempts to keep the desired replicas available

[source,bash]
----
oc describe deployment {servicename} | grep Replicas
----

== Adding Virtual Services

=== recommendation:v2

We can experiment with Istio routing rules by making a change to `RecommendationController.java` like the following and creating a "v2" docker image.

[source,java]
----
private static final String RESPONSE_STRING_FORMAT = "recommendation v2 from '%s': %d\n";
----

The "v2" tag during the Docker build is significant.

There is also a second `deployment.yml` file to label things correctly

[source,bash]
----
cd recommendation

mvn clean package

docker build -t example/recommendation:v2 .

docker images | grep recommendation
example/recommendation                  v2                  c31e399a9628        5 seconds ago       438MB
example/recommendation                  v1              f072978d9cf6        8 minutes ago      438MB
----

_Important:_ We have a 2nd Deployment to manage the v2 version of recommendation. 

[source,bash]
----
oc apply -f ./kubernetes/Deployment-v2.yml -n tutorial
oc get pods -w
----

Wait until the status is `Running` and there are `1/1` pods in the `Ready` column. To exit, press `Ctrl+C`

[source,bash]
----
NAME                                 READY     STATUS    RESTARTS   AGE
customer-3600192384-fpljb            1/1       Running   0          17m
preference-243057078-8c5hz           1/1       Running   0          15m
recommendation-v1-60483540-9snd9     1/1       Running   0          12m
recommendation-v2-2815683430-vpx4p   1/1       Running   0          15s
----

==== Add Istio sidecar to all Deployments

[source,bash]
----
cd customer
oc apply -f <(istioctl kube-inject -f ./kubernetes/Deployment.yml) -n tutorial

cd ../preference
oc apply -f <(istioctl kube-inject -f ./kubernetes/Deployment.yml) -n tutorial

cd ../recommendation
oc apply -f <(istioctl kube-inject -f ./kubernetes/Deployment.yml) -n tutorial
oc apply -f <(istioctl kube-inject -f ./kubernetes/Deployment-v2.yml) -n tutorial

oc get pods -w -n tutorial
----

Wait until the status is `Running` and there are `2/2`  customer, preferences and recommendations pods in the `Ready` column. To exit, press `Ctrl+C`

and test the customer endpoint

[source,bash]
----
curl customer-tutorial.$(minishift ip).nip.io
----

you likely see "customer =&gt; preference =&gt; recommendation v1 from '99634814-d2z2t': 3", where '99634814-d2z2t' is the pod running v1 and the 3 is basically the number of times you hit the endpoint.

[source]
----
curl customer-tutorial.$(minishift ip).nip.io
----

you likely see "customer =&gt; preference =&gt; recommendation v2 from '2819441432-5v22s': 1" as by default you get round-robin load-balancing when there is more than one Pod behind a Service

Send several requests to see their responses

[source,bash]
----
#!/bin/bash
while true
do curl customer-tutorial.$(minishift ip).nip.io
sleep .5
done
----

The default Kubernetes/OpenShift behavior is to round-robin load-balance across all available pods behind a single Service. Add another replica of recommendation-v2 Deployment.

[source,bash]
----
oc scale --replicas=2 deployment/recommendation-v2
----

Now, you will see two requests into the v2 and one for v1.

[source,bash]
----
customer => preference => recommendation v1 from '2819441432-qsp25': 29
customer => preference => recommendation v2 from '99634814-sf4cl': 37
customer => preference => recommendation v2 from '99634814-sf4cl': 38
----

Scale back to a single replica of the recommendation-v2 Deployment

[source,bash]
----
oc scale --replicas=1 deployment/recommendation-v2
----

and back to the main directory

[source,bash]
----
cd ..
----

With Istio side cars deployed to the applications, its possible to alter routing rules to services.

==== Create Destination Rules for Recommendation
[source,bash]
----
$ istioctl create -f istiofiles/destination-rule-recommendation.yml -n tutorial
----

==== All users to recommendation:v2

From the main kubeboot directory,

[source,bash]
----
istioctl create -f istiofiles/virtual-service-recommendation-v2.yml -n tutorial

curl customer-tutorial.$(minishift ip).nip.io
----

you should only see v2 being returned

==== All users to recommendation:v1

Note: "replace" instead of "create" since we are overlaying the previous rule

[source,bash]
----
istioctl replace -f istiofiles/virtual-service-recommendation-v1.yml -n tutorial

istioctl get virtualservices -n tutorial

istioctl get virtualservices recommendation-default -o yaml -n tutorial
----

==== All users to recommendation v1 and v2

By simply removing the rule

[source,bash]
----
istioctl delete virtualservice recommendation -n tutorial
istioctl create -f istiofiles/virtual-service-recommendation-v1_and_v2.yml -n tutorial
----

and you should see the default behavior of load-balancing between v1 and v2

[source,bash]
----
curl customer-tutorial.$(minishift ip).nip.io
----

==== Split traffic between v1 and v2

Canary Deployment scenario: push v2 into the cluster but slowly send end-user traffic to it, if you continue to see success, continue shifting more traffic over time

[source,bash]
----
oc get pods -l app=recommendation -n tutorial
NAME                                  READY     STATUS    RESTARTS   AGE
recommendation-v1-3719512284-7mlzw   2/2       Running   6          2h
recommendation-v2-2815683430-vn77w   2/2       Running   0          1h
----

Create the routerule that will send 90% of requests to v1 and 10% to v2

[source,bash]
----
istioctl create -f istiofiles/route-rule-recommendation-v1_and_v2.yml -n tutorial
----

and send in several requests

[source,bash]
----
#!/bin/bash
while true
do curl customer-tutorial.$(minishift ip).nip.io
sleep .1
done
----

In another terminal, change the mixture to be 75/25

[source,bash]
----
istioctl replace -f istiofiles/route-rule-recommendation-v1_and_v2_75_25.yml -n tutorial
----

Clean up

[source,bash]
----
istioctl delete virtualservice recommendation-v1-v2 -n tutorial
----

== More fun !

== Smart routing based on user-agent header (Canary Deployment)

What is your user-agent?

https://www.whoishostingthis.com/tools/user-agent/[https://www.whoishostingthis.com/tools/user-agent/]

Note: the "user-agent" header being forwarded in the Customer and Preferences controllers in order for route rule modications around recommendation

==== Set recommendation to all v1

[source,bash]
----
istioctl create -f istiofiles/virtual-service-recommendation-v1.yml -n tutorial
----

==== Set Safari users to v2

[source,bash]
----
istioctl replace -f istiofiles/virtual-service-safari-recommendation-v2.yml -n tutorial

istioctl get virtualservices -n tutorial
----

and test with a Safari (or even Chrome on Mac since it includes Safari in the string). Safari only sees v2 responses from recommendation

and test with a Firefox browser, it should only see v1 responses from recommendation.

There are two ways to get the URL for your browser:

[source,bash]
----
minishift openshift service customer --in-browser
----

That will open the openshift service `customer` in browser

Or

if you need just the url alone:

[source,bash]
----
minishift openshift service customer --url
http://customer-tutorial.192.168.99.102.nip.io
----

You can also attempt to use the curl -A command to test with different user-agent strings. 

[source,bash]
----
curl -A Safari customer-tutorial.$(minishift ip).nip.io
curl -A Firefox customer-tutorial.$(minishift ip).nip.io
----

You can describe the virtualservice to see its configuration

[source,bash]
----
istioctl get virtualservice recommendation -o yaml -n tutorial
----

Clean up

[source,bash]
----
istioctl delete virtualservice recommendation -n tutorial
----